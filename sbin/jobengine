#!/usr/bin/perl
#
# Simple job scheduler.
# Monitor $JOBQUEUE for new job files. Each jobfile spawns a process that runs
# Job->process(). The scheduler defaults to 4 concurrent processes.
#
# Soren Juul Moller, Nov 2019

use strict;
use warnings;
use Getopt::Std;
use Carp qw(longmess);
use Parallel::Fork::BossWorkerAsync ();
use Linux::Inotify2;

use FindBin;
use lib "$FindBin::Bin/../lib";
use BaseConfig;
use Utils;
use Logger;
use RinexSet;
use Job;

my %Running;
my $Debug = 1;  	# set with -d option


#####################################
# Submit a dayjob on an incomplete day
#
sub force_completion($$$) {
  my ($site, $year, $doy) = @_;

  loginfo("Force completion of $site-$year-$doy");
  my $dayjob = Job->new(site => $site, year => $year, doy => $doy);
  my $workdir = $dayjob->getWorkdir;

  # Find interval
  my $interval;
  foreach my $hour ('a'..'x') {
    next unless -f "$workdir/rs.$hour.json";
    my $rs = RinexSet->new(rsfile => "$workdir/rs.$hour.json");
    $interval = $rs->{'interval'};
    last if defined $interval;
  }

  # Submit dayjob
  if (defined $interval) {
    ($dayjob->{'hour'}, $dayjob->{'interval'}, $dayjob->{'force_complete'}) = ('0', $interval, 1);
    $dayjob->setstate('queued');
    $dayjob->submitjob('hour2daily');
  } else {
    logerror("6:Cannot force completion of $site-$year-$doy. No proccessed hours.");
  }
}


#####################################
# Look for forced incomplete days and submit them
#
sub scan_for_forced_incompletes() {
  sub dirscan($) {
    my $dir = shift;
    opendir(my $dh, $dir);
    my @dirs = grep { index($_,'.') != 0 && -d "$dir/$_" } readdir($dh);
    closedir($dh);
    return @dirs;
  }
  foreach my $site (dirscan($WORKDIR)) {
    foreach my $year (dirscan("$WORKDIR/$site")) {
      foreach my $doy (dirscan("$WORKDIR/$site/$year")) {
        my $w = "$WORKDIR/$site/$year/$doy";
        next unless -f "$w/force-complete";
        unlink("$w/force-complete");
        force_completion($site, $year, $doy);
      }
    }
  }
}


#####################################
# Move all $SAVEDIR files for a site/year/doy to $INCOMING
# The site/year/doy needs to be forgotten before calling this.
#
sub reprocess($$$) {
  my ($site, $year, $doy) = @_;

  my ($fromdoy, $todoy) = split(/-/, $doy);
  $todoy = $fromdoy unless defined $todoy;
  $todoy = $fromdoy if $todoy < $fromdoy;

  for (my $i = $fromdoy; $i <= $todoy; $i++) {
    my $savedir = sprintf("%s/%s/%4d/%03d", $SAVEDIR, $site, $year, $i);
    if (!-d $savedir) {
      logwarn("Cannot reprocess $site-$year-$doy. Not in $SAVEDIR.");
    } else {
      sysmv([<"$savedir/*">], $INCOMING, { log => $Debug });
    }
  }
}


#####################################
# Run command issued by someone like admin.cgi
#
sub runcommand($) {
  my $cmd = shift;

  if ($cmd =~ /^reload ftpuploader/) {
    sysrun([qw(/usr/bin/systemctl reload ftpuploader)], { log => 1 });
  }
  elsif ($cmd =~ /^force complete (\w+) ([0-9]+) ([0-9]+)/) {
    force_completion($1, $2, $3);
  }
  elsif ($cmd =~ /^reprocess (\w+) ([0-9]+) ([0-9\-]+)/) {
    reprocess($1, $2, $3);
  }
}


#####################################
# This sub is runned by the fork manager. Remember, this is another process than the main in this script.
# This sub calls the main sub in Job.pm that does all the work.
#
sub runjob($) {
  my $jobfileref = shift;
  my ($jobfile, $content) = ($jobfileref->{'jobfile'}, $jobfileref->{'content'});

  # If filename ends with 'command', then assume it is a command file
  if ($jobfile =~ /command$/) {
    # message from admin.cgi
    runcommand($content);
    return { ident => 'command', result => 'ok' };
  }

  # otherwise assume it's a JSON file
  my $job = Job->new(json => $content);
  my $ident = $job->getIdent();
  my $res = { ident => $ident, result => 'error' };

  unless (defined $job && $job->verifyobj()) {
    logerror("invalid jobfile $jobfile");
    return $res;
  }

  # Duplicate event check
  if (exists $Running{$ident}) {
    logerror("Duplicate job $ident");
    return $res;
  }

  my $workdir = $job->getWorkdir;
  chdir($workdir);

  # Set state to running
  my $state = $job->lockstate()->readstate();
  if ($state ne 'queued') {
    $job->unlockstate();
    logerror("$ident: Illegal state: $state");
    return $res;
  }
  $job->writestate('running')->unlockstate();

  loginfo("Run $ident");
  eval {
    $job->process();
    1;
  } || do {
    logerror("FATAL ERROR: ".longmess($ident));	# Serious program error occurred.
    $res->{'result'} = 'fatal';
    return $res;
  };

  $res->{'result'} = 'ok';
  return $res;
}


#
# Initialize BossWorkerAsync
# See 'man Parallel::Fork::BossWorkerAsync
#
sub start_bw($) {
  my $instances = shift;
  return Parallel::Fork::BossWorkerAsync->new(
	work_handler => \&runjob,
	init_handler => sub { setprogram('job'); $0 = 'job' },
	global_timeout => 0,
	worker_count => $instances,
	read_size => 1024 * 2,
  );
}


########################################################################################################
# Main program
#
my %opts = ();
getopts('c:di:l:', \%opts);

BaseConfig::init($opts{'c'}) if defined $opts{'c'};

my $log = (defined $opts{'l'} ? $opts{'l'} : "/dev/null");
$Debug = 1 if defined $opts{'d'};

my $Ninstances = (defined $opts{'i'} ? $opts{'i'} : $JOBINSTANCES);

setprogram($0);
$0 = basename($0)." ".join(' ',@ARGV);

my $BW = start_bw($Ninstances);
loginfo("jobengine started with $Ninstances instances");

# Setup queue event handler
my $inotify = Linux::Inotify2->new();
$inotify->blocking(0);
my $watcher = $inotify->watch($JOBQUEUE, IN_MOVED_TO | IN_CLOSE_WRITE);

# main loop
my $signal_received = '';
$SIG{INT} = sub { $signal_received = 'interrupt' };
$SIG{TERM} = sub { $signal_received = 'terminated' };
$SIG{HUP} = 'IGNORE';

# Contains jobfiles ready to read.
# Index is full name and value is time arrived
my %RunQueue;

# Enqueue jobs already in jobspool
foreach (dirlist($JOBQUEUE)) {
  my $fn = "$JOBQUEUE/$_";
  $RunQueue{$fn} = 0;
  unlink($fn);
}

my $ntimeouts = 0;
my $need_restart = 0;

while (!$signal_received) {
  my @runq = sort { $RunQueue{$a} - $RunQueue{$b} } keys %RunQueue;
  foreach (@runq) {
    next if time() - $RunQueue{$_} < 2;		# Delay processing by at least 1s
    my $content = readfile($_);
    if (defined $content && length($content) > 0) {
      $BW->add_work({ jobfile => $_, content => $content });
    } else {
      logerror("$_ does not exist or empty?!");
    }
    unlink($_);
    delete $RunQueue{$_};
  }

  my $v = '';
  vec($v, $inotify->fileno, 1) = 1;
  select($v, undef, undef, 1);			# wait 1s for inotify has something
  if (vec($v, $inotify->fileno, 1)) {
    $ntimeouts = 0;
    # Read all events available
    $RunQueue{$_->fullname} = time() foreach $inotify->read;
  } elsif (scalar(keys %RunQueue) == 0) {
    $ntimeouts++;
    if ($BW->pending()) {			# Do we have outstanding results?
      my $res = $BW->get_result_nb();		# Try get a result
      next unless defined $res;			# Wait for next file if nothing yet...
      logerror($res->{'ERROR'}) if $res->{'ERROR'};
      delete $Running{$res->{'ident'}};
      if ($res->{'result'} eq 'fatal') {
        $need_restart = 1;
      }
    }
    if ($ntimeouts % 600 == 0) {		# For every 10min idle time, check for leftovers
      foreach (dirlist($JOBQUEUE)) {
        my $fn = "$JOBQUEUE/$_";
        next if fileage($fn) < 900;		# Wait at least 15min before running a leftover
        $RunQueue{$fn} = 0;
        unlink($fn);
      }
      scan_for_forced_incompletes();
    }
    if ($need_restart) {
      # Something bad happened. Restart fork manager...
      loginfo("Restarting fork manager due to fatal error");
      $watcher->cancel;
      $BW->shut_down(force => 1);
      sleep 300;
      $BW = start_nw($Ninstances);
      $watcher = $inotify->watch($JOBQUEUE, IN_MOVED_TO | IN_CLOSE_WRITE);
      $need_restart = 0;
      loginfo("Fork manager restarted.");
    }
  }
}
loginfo($signal_received) if $signal_received;

$watcher->cancel;
$BW->shut_down();

loginfo("$0 stopped");
logclose();
exit(0);
